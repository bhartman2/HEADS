---
title: "Chapter-8"
author: "[Bruce Hartman](https://drbrucehartman.net/brucewebsite/)"
date: "Last updated: `r format(Sys.time(), '%d %B, %Y')`"
output: html_notebook
---

# Introduction
Chapter 8 discussion from [the HEADS text](#r1).

# Packages
Regular R packages:
```{r}
library(tidyverse)
library(ggfortify)
library(GGally)
library(broom)
library(skimr)
library(gt)
library(patchwork)
library(yardstick)
library(car)
```

Github packages:
```{r}
library(devtools)
install_github("bhartman2/mbadtools")
library(mbadtools)
```

To install this package: use uncommented in your notebook.
```{r}
# install_github("bhartman2/HEADS")
# library(HEADS)
```

# Chapter 8
We work through some of the text examples from [the book](#r1).

# Trees (8.1)

## Data

## Example 8.1.2
Data come from Table 8.1 of [the HEADS text](#r1)
```{r}
data(Table_8_1)
Table_8_1 %>% glimpse
```

### Trees
Tree: we try an hclust tree, since it is easy.

```{r} 
h = hclust(dist(Table_8_1))
ch = cutree(h,k=4 )
ch
dfh = data.frame(Table_8_1, Cluster=ch)
ggplot(dfh, aes(BMI, GlucoseLevel))+
  geom_point(aes(color=factor(Cluster), shape=factor(Diabetes)), size=3)
table(Table_8_1$Diabetes, factor(ch))
```

`rpart` trees are better:
```{r}
library(rpart)
library(rpart.plot)
```

For classification splitting, the list can contain any of: the vector of prior probabilities (component prior), the loss matrix (component loss) or the splitting index (component split). The priors must be positive and sum to 1. The loss matrix must have zeros on the diagonal and positive off-diagonal elements. The splitting index can be gini or information. The default priors are proportional to the data counts, the losses default to 1, and the split defaults to gini.

Priors:
```{r}
table(Table_8_1$Diabetes)
Table_8_1 %>% group_by(Diabetes) %>% summarize(n=n())
```

```{r}
tree = rpart(Diabetes ~ GlucoseLevel, data=Table_8_1, method="class")
tree
rpart.plot(tree, extra=102)
```
The `rpart` tree gives a split at 153 for Glucose

```{r}
tree = rpart(Diabetes ~ BMI, data=Table_8_1, method="class")
tree
rpart.plot(tree, extra=102)
```
The `rpart` tree gives a split at 29 for BMI. Does it match the book computation?

### Book method; ginit() function
```{r}
z = Table_8_1 %>% mutate(x=(BMI<25.7))
z
q = table(z$x, z$Diabetes) %>% addmargins
q
```

```{r}
GiniBMI = (q[2,2]/q[2,3])*(1-(q[2,2]/q[2,3])) + (q[1,2]/q[1,3])*(1-(q[1,2]/q[1,3])) +
          (q[2,1]/q[2,3])*(1-(q[2,1]/q[2,3])) + (q[1,1]/q[1,3])*(1-(q[1,1]/q[1,3]))
GiniBMI

giniBMI1 = ginit(q)
giniBMI1
```

```{r}
z2 = Table_8_1 %>% mutate(x2=(GlucoseLevel<159))
z2
q2 = table(z2$x2, z2$Diabetes) %>% addmargins
q2
ginit(q2)
```

### Gini search
Functions implemented using `ginit`.

```{r warning=FALSE}
gs = ginisearch(Table_8_1, "GlucoseLevel", "Diabetes")
gs %>% plot_ginisearch + labs(x="GlucoseLevel")
```

```{r warning=FALSE}
gs = ginisearch(Table_8_1, "BMI", "Diabetes")
gs %>% plot_ginisearch + labs(x="BMI")
```

These give very close to [the book](#r1) answers.

### DescTools Gini Index
Gini index:
```{r}
library(DescTools)
# t = table(Table_8_1$GlucoseLevel)
z2
(z2$x2)*1
t = table((z2$x2)*1, z2$Diabetes) %>% addmargins
t %>% str
t
tt = t %>% enframe(name="var", value="nn") %>% mutate_all(as.numeric)
tt

# weighted by value weighted
Gini(tt$var, tt$nn, unbiased=FALSE)
```
Does not match book or `rpart` or ginit methods.

## Bagging


# Support Vector Machines (8.2)
We use the `kernlab` package.
```{r}
library(kernlab)
```


## Data linearly separable
Linearly separable data:
```{r}
data(Table_8_2)
Table_8_2
df = Table_8_2 %>% mutate(Diabetes = case_when(Diabetes=="negative" ~ -1,
                                               .default = 1))
df
```
### SVM fit
```{r}
svmfit = ksvm(Diabetes~GlucoseLevel+BMI, data=df, scaled=FALSE, 
              type="C-svc", kernel="vanilladot")
# svmfit %>% glimpse
# outputs from svm
cat("b=", svmfit@b)
cat("\ncoef=",paste(svmfit@coef))
cat("\nalpha=", paste(svmfit@alpha))
cat("\nobj=", svmfit@obj)
cat("\nnSV=", svmfit@nSV)
cat("\nalphaindex=", paste(svmfit@alphaindex))
# Get the indices of support vectors
support_vector_indices <- svmfit@SVindex
cat("\nSVindex=", support_vector_indices)
# Print support vectors
df3[support_vector_indices, ]
cat("\n")
# book w* and calculated objective on p 336.
bstar = -7.93
wstar=c(.0625, -.001763); 
cat("\nwstar= ",wstar)
cat("\nCalculated Objective from wstar= ", (1/2)*( wstar[1]^2 + wstar[2]^2 ) )
```

`b` matches the book. `obj` almost matches the calculated objective from the book.

### Figure

```{r fig.width=12}
plot(svmfit, data=df, xlim=c(25,50), ylim=c(60,200) )
points(df %>% slice(support_vector_indices))
```

```{r}
gg_plotsvm_unscaled(svmfit, df, BMI, GlucoseLevel, color=factor(Diabetes))
```

### Prediction
Predictions:
```{r}
pr = predict(svmfit, type="response")
table(df$Diabetes, pr)
```
Perfect prediction! The data are linearly separable.

### Figure 8.9
Emulate Figure 8.9:
```{r}
s = svmfit@obj/wstar[2]; s
int = wstar[1]/wstar[2]; int
ggplot(df) +
  geom_point(aes(BMI,GlucoseLevel, color=as.factor(Diabetes)), shape=9, size=3) +
  # geom_abline(aes(intercept=int,
                  # slope = s), color="black") +
  # coord_cartesian(xlim=c(0,200), ylim=c(0,50))
  theme_bw()
```

Obviously we do not know how to calculate the support vector.

## Data not linearly separable:
```{r}
data(Table_8_3)
df3 = Table_8_3 %>% mutate(Diabetes = case_when(Diabetes=="negative" ~ -1,
                                               .default = 1))
df3
```

### SVM fit
We need to use a soft margin fit, controlled by the C parameter. The objective value will change too.
```{r}
svmfit3 = ksvm(Diabetes~GlucoseLevel+BMI, data=df3, scaled=FALSE, 
              type="C-svc", kernel="vanilladot", 
              C=0.1)
# svmfit3 %>% glimpse
# outputs from svm
cat("b=", svmfit3@b)
cat("\ncoef=",paste(svmfit3@coef))
cat("\nalpha=", paste(svmfit3@alpha))
cat("\nalphaindex=", paste(svmfit3@alphaindex))
# Get the indices of support vectors
support_vector_indices3 <- svmfit3@SVindex
cat("\nSVindex=", support_vector_indices3)
# Print support vectors
df3[support_vector_indices3, ]
cat("\nobj=", svmfit3@obj)
cat("\nnSV=", svmfit3@nSV)
cat("\n")
# book w* and calculated objective on p 336.
bstar3 = -12.83
wstar3=c(.0495, .194); 
cat("\nwstar= ",wstar3)
cat("\nCalculated Objective from wstar= ", (1/2)*( wstar3[1]^2 + wstar3[2]^2 ) )
cat("\nOmits the soft adjustment term")
```
`b` matches the book again!!. `obj` does not match the calculated objective from the book. 

### Figure 8.11
```{r fig.width=12}
plot(svmfit3, data=df3, xlim=c(20,50), ylim=c(60,200) )
points(df3 %>% slice(support_vector_indices3))
```

```{r fig.width=12}
gg_plotsvm(svmfit3, df3, 
          GlucoseLevel, BMI, color=factor(Diabetes)) +
  gg_plotsvm_contour(grid=svmgrid(df3, svmfit3), legend=T, bins=10) +
  # gg_plotsvm_margin(svmmargin(svmfit3), x=150, y=42) +
  # gg_plotsvm_marginlines(svmfit3)
```
```{r}
P = gg_plotsvm(svmfit3, df3, 
          GlucoseLevel, BMI, color=factor(Diabetes)) +
  gg_plotsvm_contour(grid=svmgrid(df3, svmfit3), legend=T, bins=10) 
P
#under development
  P + 
    gg_plotsvm_margin(svmmargin(svmfit3), 150, 45) +
    gg_plotsvm_marginlines(svmfit3)
```

# Confusion Matrix (8.3)
We create a confusion matrix for the hclust example above.
```{r}
# original df
dfh %>% glimpse
#make table with truth first
cm = table(dfh$Diabetes, dfh$Cluster); cm
# reclassify Cluster 1 and 2 to "neg, and the rest to "pos"
dfh = dfh %>% mutate(NewClus = factor(case_when(Cluster <= 2 ~ "negative",
                                              .default = "positive")))
cm1 = table(dfh$Diabetes, dfh$NewClus)
cm1
cm1 %>% conf_mat %>% autoplot(type="heatmap") +
  scale_fill_distiller(palette="Paired")
# get performance metrics
caret::confusionMatrix(cm1)
```

## ROC and AUC
```{r}
# Must create a probability model rather than discrete values.
svmfit4 = ksvm(Diabetes~GlucoseLevel+BMI, data=df3, scaled=FALSE, 
              type="C-svc", kernel="vanilladot", prob.model=TRUE,
              C=0.1)

library(pROC)
# compute predicted probabilities
predicted_probs = predict(svmfit4, newdata=df3, 
                          type="probabilities")
# create roc object
roc_obj = roc(df3$Diabetes, predicted_probs[,2],
              direction="<", ci=T)
# compute AUC
auc_value = auc(roc_obj)
cat("\nAUC value= ", auc_value)
# plot the ROC curve
plot(roc_obj, col="blue", main="ROC Curve", xlim=c(0,1), ylim=c(0,1))
text(x=1, y=1, 
     labels=paste0("\nAUC value= ", auc_value)) 
```
```{r}
roc_obj %>% glimpse
dfroc = tibble(specificities=roc_obj$specificities,
               sensitivities=roc_obj$sensitivities)
ggplot(dfroc) +
  geom_line(aes(specificities, sensitivities), color="blue") +
  geom_abline(aes(intercept=1, slope=-1), 
              linetype="dotdash", alpha=.4) +
  annotate("text", x=.5, y=1, 
           label=paste0("\nAUC value= ", auc_value))+
  theme_minimal()
```


# References
:::{#r1}
Mehrotra, Sanjay, Kevin Bui, and Hari Balasubramanian (2024). Healthcare Engineering,
Analytics, and Decision Sciences: An Introduction. December 11, 2024.
(https://sites.google.com/view/healthcareengineering/)
:::

:::{#r2}
Machine Learning with Classification (2024) Chapter 11: Trees and Classification. (https://fderyckel.github.io/machinelearningwithr/trees-and-classification.html)
:::

<!---
:::{#r3}
Medical Expenditure Panel Survey (MEPS) (2024) (https://meps.ahrq.gov/mepsweb/)
:::

--->
